{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLOB: Deep Convolutional Neural Networks for Limit Order Books\n",
    "\n",
    "### Authors: Zihao Zhang, Stefan Zohren and Stephen Roberts\n",
    "Oxford-Man Institute of Quantitative Finance, Department of Engineering Science, University of Oxford\n",
    "\n",
    "This jupyter notebook is used to demonstrate our recent paper [2] published in IEEE Transactions on Singal Processing. We use FI-2010 [1] dataset and present how model architecture is constructed here. \n",
    "\n",
    "### Data:\n",
    "The FI-2010 is publicly avilable and interested readers can check out their paper [1]. The dataset can be downloaded from: https://etsin.fairdata.fi/dataset/73eb48d7-4dbc-4a10-a52a-da745b47a649 \n",
    "\n",
    "Otherwise, the notebook will download the data automatically or it can be obtained from: \n",
    "\n",
    "https://drive.google.com/drive/folders/1Xen3aRid9ZZhFqJRgEMyETNazk02cNmv?usp=sharing.\n",
    "\n",
    "### References:\n",
    "[1] Ntakaris A, Magris M, Kanniainen J, Gabbouj M, Iosifidis A. Benchmark dataset for midâ€price forecasting of limit order book data with machine learning methods. Journal of Forecasting. 2018 Dec;37(8):852-66. https://arxiv.org/abs/1705.03233\n",
    "\n",
    "[2] Zhang Z, Zohren S, Roberts S. DeepLOB: Deep convolutional neural networks for limit order books. IEEE Transactions on Signal Processing. 2019 Mar 25;67(11):3001-12. https://arxiv.org/abs/1808.03668\n",
    "\n",
    "### This notebook runs on tensorflow 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # obtain data\n",
    "# import os \n",
    "# if not os.path.isfile('data.zip'):\n",
    "#     !wget https://raw.githubusercontent.com/zcakhaa/DeepLOB-Deep-Convolutional-Neural-Networks-for-Limit-Order-Books/master/data/data.zip\n",
    "#     !unzip -n data.zip\n",
    "#     print('data downloaded.')\n",
    "# else:\n",
    "#     print('data already existed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# limit gpu memory\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "##############\n",
    "## my imports\n",
    "##############\n",
    "from glob import glob\n",
    "from json import loads\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "##############\n",
    "import random\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Flatten, Dense, Dropout, Activation, Input, LSTM, Reshape, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set random seeds\n",
    "# np.random.seed(int(time()))\n",
    "# tf.random.set_seed(int(time()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "We used no auction dataset that is normalised by decimal precision approach in their work. The first 40 columns of the FI-2010 dataset are 10 levels ask and bid information for a limit order book and we only use these 40 features in our network. The last 5 columns of the FI-2010 dataset are the labels with different prediction horizons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_x(data):\n",
    "    df1 = data[:40, :].T\n",
    "    return np.array(df1)\n",
    "\n",
    "def get_label(data):\n",
    "    lob = data[-5:, :].T\n",
    "    return lob\n",
    "\n",
    "def data_classification(X, Y, T):\n",
    "    [N, D] = X.shape\n",
    "    df = np.array(X)\n",
    "    dY = np.array(Y)\n",
    "    dataY = dY[T - 1:N]\n",
    "    dataX = np.zeros((N - T + 1, T, D))\n",
    "    for i in range(T, N + 1):\n",
    "        dataX[i - T] = df[i - T:i, :]\n",
    "    return dataX.reshape(dataX.shape + (1,)), dataY\n",
    "\n",
    "def prepare_x_y(data, k, T):\n",
    "    x = prepare_x(data)\n",
    "    y = get_label(data)\n",
    "    x, y = data_classification(x, y, T=T)\n",
    "    y = y[:,k] - 1\n",
    "    y = np_utils.to_categorical(y, 3)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('./data/LOBdata*.csv')\n",
    "dfs = [pd.read_csv(i) for i in files]\n",
    "coins = np.ndarray.tolist(pd.concat(dfs).coin.unique())\n",
    "coinseperate = {}\n",
    "coinslabel = {}\n",
    "for coin in coins:\n",
    "    maskdfs = []\n",
    "    for df in dfs:\n",
    "        if coin in df['coin'].to_numpy():\n",
    "            maskdfs.append(df)\n",
    "    labels = [df[df['coin'] == coin].iloc[0].label for df in maskdfs]\n",
    "    coinslabel[coin] = labels\n",
    "    coinseperate[coin] = [df[df['coin'] == coin].iloc[0].matrix for df in maskdfs]\n",
    "\n",
    "for coin in coins:\n",
    "    x = np.stack([loads(i) for i in coinseperate[coin]])\n",
    "    x = x.reshape(list(x.shape)+ [1])\n",
    "    coinseperate[coin] = x\n",
    "    labels_index = list(map(lambda x:x+1,coinslabel[coin]))\n",
    "    labels = []\n",
    "    for i in labels_index:\n",
    "        l = [0]*3\n",
    "        l[int(i)] = 1\n",
    "        labels.append(l)\n",
    "    coinslabel[coin] = np.array(labels)\n",
    "\n",
    "keys = coins.copy()\n",
    "allowed = ['ADA', 'XRP', 'SOL', 'DOGE', 'DOT', 'TRX', 'SHIB', 'AVAX', 'LTC', 'FTT', 'MATIC',\n",
    " 'LINK', 'UNI', 'XLM', 'NEAR', 'BCH', 'ALGO', 'XMR', 'ETC', 'ATOM', 'VET', 'MANA',\n",
    " 'HBAR', 'FLOW', 'HNT', 'ICP', 'TUSD', 'XTZ', 'THETA', 'FIL', 'EGLD', 'SAND', 'APE',\n",
    " 'USDP', 'BTTC', 'EOS', 'ZEC', 'AXS', 'AAVE', 'IOTA', 'MKR', 'XEC', 'GRT']\n",
    "\n",
    "# for coin in keys:\n",
    "#     if coin not in allowed:\n",
    "#         coins.remove(coin)\n",
    "\n",
    "# coinseperate['CITY'].shape\n",
    "trainX_CNN = np.concatenate([coinseperate[coin] for coin in coins])\n",
    "trainY_CNN = np.concatenate([coinslabel[coin] for coin in coins])\n",
    "# trainX_CNN = trainX_CNN.reshape([trainX_CNN.shape[0]*trainX_CNN.shape[1]]+list(trainX_CNN.shape[2:]))\n",
    "# trainY_CNN = trainY_CNN.reshape([trainY_CNN.shape[0]*trainY_CNN.shape[1]]+list(trainY_CNN.shape[2:]))\n",
    "\n",
    "musk2 = [i[2]==1 for i in trainY_CNN]\n",
    "ex2 = trainX_CNN[musk2]\n",
    "lab2 = trainY_CNN[musk2]\n",
    "\n",
    "musk1 = [i[1]==1 for i in trainY_CNN]\n",
    "ex1 = trainX_CNN[musk1]\n",
    "lab1 = trainY_CNN[musk1]\n",
    "\n",
    "musk0 = [i[0]==1 for i in trainY_CNN]\n",
    "ex0 = trainX_CNN[musk0]\n",
    "lab0 = trainY_CNN[musk0]\n",
    "\n",
    "ex12 = []\n",
    "lab12 = []\n",
    "ex1 = np.ndarray.tolist(ex1)\n",
    "lab1 = np.ndarray.tolist(lab1)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(ex2)):\n",
    "    index = random.randint(0,len(ex1)-1)\n",
    "    ex12.append(ex1.pop(index))\n",
    "    lab12.append(lab1.pop(index))\n",
    "\n",
    "ex1 = np.array(ex12)\n",
    "lab1 = np.array(lab12)\n",
    "    \n",
    "trainX_CNN = np.concatenate([ex0,ex1,ex2])\n",
    "trainY_CNN = np.concatenate([lab0,lab1,lab2])\n",
    "\n",
    "\n",
    "n_hiddens = 64\n",
    "checkpoint_filepath = './model_tensorflow2/weights'\n",
    "trainX_CNN.shape,trainY_CNN.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split( trainX_CNN, trainY_CNN,stratify=trainY_CNN, test_size=0.2,shuffle=True)\n",
    "\n",
    "trainX_CNN = X_train\n",
    "trainY_CNN = y_train\n",
    "\n",
    "testX_CNN = X_test\n",
    "testY_CNN = y_test\n",
    "batch_size = 32\n",
    "decay_epoch = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2350, 100, 40, 1), (588, 100, 40, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX_CNN.shape,testX_CNN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # please change the data_path to your local path\n",
    "# # data_path = '/nfs/home/zihaoz/limit_order_book/data'\n",
    "\n",
    "# dec_data = np.loadtxt('Train_Dst_NoAuction_DecPre_CF_7.txt')\n",
    "# dec_train = dec_data[:, :int(np.floor(dec_data.shape[1] * 0.8))]\n",
    "# dec_val = dec_data[:, int(np.floor(dec_data.shape[1] * 0.8)):]\n",
    "\n",
    "# dec_test1 = np.loadtxt('Test_Dst_NoAuction_DecPre_CF_7.txt')\n",
    "# dec_test2 = np.loadtxt('Test_Dst_NoAuction_DecPre_CF_8.txt')\n",
    "# dec_test3 = np.loadtxt('Test_Dst_NoAuction_DecPre_CF_9.txt')\n",
    "# dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "\n",
    "# k = 4 # which prediction horizon\n",
    "# T = 100 # the length of a single input\n",
    "# n_hiddens = 64\n",
    "# checkpoint_filepath = './model_tensorflow2/weights'\n",
    "\n",
    "# trainX_CNN, trainY_CNN = prepare_x_y(dec_train, k, T)\n",
    "# valX_CNN, valY_CNN = prepare_x_y(dec_val, k, T)\n",
    "# testX_CNN, testY_CNN = prepare_x_y(dec_test, k, T)\n",
    "\n",
    "# print(trainX_CNN.shape, trainY_CNN.shape)\n",
    "# print(valX_CNN.shape, valY_CNN.shape)\n",
    "# print(testX_CNN.shape, testY_CNN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "Please find the detailed discussion of our model architecture in our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 100, 40, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " rescaling_3 (Rescaling)        (None, 100, 40, 1)   0           ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 100, 20, 32)  96          ['rescaling_3[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_42 (LeakyReLU)     (None, 100, 20, 32)  0           ['conv2d_42[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 100, 20, 32)  4128        ['leaky_re_lu_42[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_43 (LeakyReLU)     (None, 100, 20, 32)  0           ['conv2d_43[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 100, 20, 32)  4128        ['leaky_re_lu_43[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_44 (LeakyReLU)     (None, 100, 20, 32)  0           ['conv2d_44[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 100, 10, 32)  2080        ['leaky_re_lu_44[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_45 (LeakyReLU)     (None, 100, 10, 32)  0           ['conv2d_45[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 100, 10, 32)  4128        ['leaky_re_lu_45[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_46 (LeakyReLU)     (None, 100, 10, 32)  0           ['conv2d_46[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 100, 10, 32)  4128        ['leaky_re_lu_46[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_47 (LeakyReLU)     (None, 100, 10, 32)  0           ['conv2d_47[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 100, 1, 32)   10272       ['leaky_re_lu_47[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_48 (LeakyReLU)     (None, 100, 1, 32)   0           ['conv2d_48[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 100, 1, 32)   4128        ['leaky_re_lu_48[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_49 (LeakyReLU)     (None, 100, 1, 32)   0           ['conv2d_49[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 100, 1, 32)   4128        ['leaky_re_lu_49[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_50 (LeakyReLU)     (None, 100, 1, 32)   0           ['conv2d_50[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 100, 1, 64)   2112        ['leaky_re_lu_50[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 100, 1, 64)   2112        ['leaky_re_lu_50[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_51 (LeakyReLU)     (None, 100, 1, 64)   0           ['conv2d_51[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_53 (LeakyReLU)     (None, 100, 1, 64)   0           ['conv2d_53[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 100, 1, 32)  0           ['leaky_re_lu_50[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 100, 1, 64)   12352       ['leaky_re_lu_51[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 100, 1, 64)   20544       ['leaky_re_lu_53[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 100, 1, 64)   2112        ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " leaky_re_lu_52 (LeakyReLU)     (None, 100, 1, 64)   0           ['conv2d_52[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_54 (LeakyReLU)     (None, 100, 1, 64)   0           ['conv2d_54[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_55 (LeakyReLU)     (None, 100, 1, 64)   0           ['conv2d_55[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 100, 1, 192)  0           ['leaky_re_lu_52[0][0]',         \n",
      "                                                                  'leaky_re_lu_54[0][0]',         \n",
      "                                                                  'leaky_re_lu_55[0][0]']         \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 100, 192)     0           ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 100, 192)     0           ['reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 64)           65792       ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 3)            195         ['lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 142,435\n",
      "Trainable params: 142,435\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_deeplob(T, NF, number_of_lstm):\n",
    "    input_lmd = Input(shape=(T, NF, 1))\n",
    "    \n",
    "    # build the convolutional block\n",
    "    conv_first1 = tf.keras.layers.Rescaling(1e-3)(input_lmd)\n",
    "    conv_first1 = Conv2D(32, (1, 2), activation='gelu',strides=(1, 2))(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1),activation='gelu', padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1),activation='gelu', padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    conv_first1 = Conv2D(32, (1, 2),activation='gelu', strides=(1, 2))(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1),activation='gelu', padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1),activation='gelu', padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    conv_first1 = Conv2D(32, (1, 10),activation='gelu')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1),activation='gelu', padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1),activation='gelu', padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    \n",
    "    # build the inception module\n",
    "    convsecond_1 = Conv2D(64, (1, 1),activation='gelu', padding='same')(conv_first1)\n",
    "    convsecond_1 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "    convsecond_1 = Conv2D(64, (3, 1),activation='gelu', padding='same')(convsecond_1)\n",
    "    convsecond_1 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "\n",
    "    convsecond_2 = Conv2D(64, (1, 1),activation='gelu', padding='same')(conv_first1)\n",
    "    convsecond_2 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "    convsecond_2 = Conv2D(64, (5, 1),activation='gelu', padding='same')(convsecond_2)\n",
    "    convsecond_2 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "\n",
    "    convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1), padding='same')(conv_first1)\n",
    "    convsecond_3 = Conv2D(64, (1, 1),activation='gelu', padding='same')(convsecond_3)\n",
    "    convsecond_3 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_3)\n",
    "    \n",
    "    convsecond_output = keras.layers.concatenate([convsecond_1, convsecond_2, convsecond_3], axis=3)\n",
    "    conv_reshape = Reshape((int(convsecond_output.shape[1]), int(convsecond_output.shape[3])))(convsecond_output)\n",
    "    conv_reshape = keras.layers.Dropout(0.2, noise_shape=(None, 1, int(conv_reshape.shape[2])))(conv_reshape, training=True)\n",
    "\n",
    "    # build the last LSTM layer\n",
    "    conv_lstm = LSTM(number_of_lstm)(conv_reshape)\n",
    "\n",
    "    # build the output layer\n",
    "    out = Dense(3, activation='softmax')(conv_lstm)\n",
    "    model = Model(inputs=input_lmd, outputs=out)\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=(trainX_CNN.shape[0]//batch_size)*100,decay_rate=0.9)\n",
    "    adam =  Adam(learning_rate=1e-6)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy',tf.keras.metrics.Precision()])\n",
    "    return model,adam\n",
    "\n",
    "deeplob,adam = create_deeplob(trainX_CNN.shape[1], trainX_CNN.shape[2], n_hiddens)\n",
    "deeplob.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1069 - accuracy: 0.3326 - precision_3: 0.3723WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "74/74 [==============================] - 5s 40ms/step - loss: 1.1076 - accuracy: 0.3319 - precision_3: 0.3608\n",
      "Epoch 2/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1054 - accuracy: 0.3309 - precision_3: 0.3636WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 1.1055 - accuracy: 0.3315 - precision_3: 0.3636\n",
      "Epoch 3/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1060 - accuracy: 0.3562 - precision_3: 0.3333WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 1.1064 - accuracy: 0.3553 - precision_3: 0.3333\n",
      "Epoch 4/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1042 - accuracy: 0.3476 - precision_3: 0.4271WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 1.1042 - accuracy: 0.3494 - precision_3: 0.4271\n",
      "Epoch 5/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1041 - accuracy: 0.3480 - precision_3: 0.3151WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 1.1046 - accuracy: 0.3477 - precision_3: 0.3108\n",
      "Epoch 6/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1073 - accuracy: 0.3557 - precision_3: 0.3140WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 1.1067 - accuracy: 0.3553 - precision_3: 0.3295\n",
      "Epoch 7/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1082 - accuracy: 0.3467 - precision_3: 0.3750WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 1.1081 - accuracy: 0.3485 - precision_3: 0.3750\n",
      "Epoch 8/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1082 - accuracy: 0.3408 - precision_3: 0.2857WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 1.1083 - accuracy: 0.3404 - precision_3: 0.2857\n",
      "Epoch 9/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1051 - accuracy: 0.3527 - precision_3: 0.2857WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 1.1050 - accuracy: 0.3528 - precision_3: 0.2857\n",
      "Epoch 10/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1085 - accuracy: 0.3420 - precision_3: 0.3556WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 1.1085 - accuracy: 0.3430 - precision_3: 0.3556\n",
      "Epoch 11/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1041 - accuracy: 0.3583 - precision_3: 0.3864WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 1.1042 - accuracy: 0.3587 - precision_3: 0.3864\n",
      "Epoch 12/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1034 - accuracy: 0.3493 - precision_3: 0.3810WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 1.1034 - accuracy: 0.3494 - precision_3: 0.3810\n",
      "Epoch 13/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1061 - accuracy: 0.3609 - precision_3: 0.3297WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 1.1061 - accuracy: 0.3596 - precision_3: 0.3297\n",
      "Epoch 14/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1074 - accuracy: 0.3476 - precision_3: 0.3372WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 1.1075 - accuracy: 0.3468 - precision_3: 0.3333\n",
      "Epoch 15/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1026 - accuracy: 0.3592 - precision_3: 0.3333WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 1.1029 - accuracy: 0.3583 - precision_3: 0.3295\n",
      "Epoch 16/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1059 - accuracy: 0.3532 - precision_3: 0.4167WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 1.1056 - accuracy: 0.3540 - precision_3: 0.4235\n",
      "Epoch 17/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1041 - accuracy: 0.3574 - precision_3: 0.4000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "74/74 [==============================] - 3s 37ms/step - loss: 1.1040 - accuracy: 0.3574 - precision_3: 0.4000\n",
      "Epoch 18/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1110 - accuracy: 0.3545 - precision_3: 0.2955WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "74/74 [==============================] - 3s 38ms/step - loss: 1.1110 - accuracy: 0.3540 - precision_3: 0.2955\n",
      "Epoch 19/150\n",
      "21/74 [=======>......................] - ETA: 1s - loss: 1.1120 - accuracy: 0.3527 - precision_3: 0.2692"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Itay\\.conda\\envs\\envi\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Itay\\.conda\\envs\\envi\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Itay\\.conda\\envs\\envi\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Itay\\.conda\\envs\\envi\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Itay\\.conda\\envs\\envi\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Itay\\.conda\\envs\\envi\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2957\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2959\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Itay\\.conda\\envs\\envi\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1854\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Itay\\.conda\\envs\\envi\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    505\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\Users\\Itay\\.conda\\envs\\envi\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 55\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "with tf.device('/GPU:0'):\n",
    "    deeplob.fit(trainX_CNN, trainY_CNN, #validation_data=(valX_CNN, valY_CNN), \n",
    "                epochs=150, batch_size=32, verbose=1,shuffle=True, callbacks=[model_checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n",
      "2 2\n",
      "2 2\n",
      "0 2\n",
      "0 2\n",
      "2 1\n",
      "1 2\n",
      "2 2\n",
      "0 2\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "0 2\n",
      "2 0\n",
      "2 2\n",
      "1 2\n",
      "2 0\n",
      "2 1\n",
      "2 0\n",
      "0 2\n",
      "2 1\n",
      "2 0\n",
      "2 0\n",
      "2 2\n",
      "2 1\n",
      "2 2\n",
      "2 0\n",
      "2 1\n",
      "0 2\n",
      "1 2\n",
      "2 2\n",
      "2 0\n",
      "2 0\n",
      "2 2\n",
      "2 1\n",
      "2 2\n",
      "2 1\n",
      "2 1\n",
      "2 0\n",
      "0 2\n",
      "0 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 0\n",
      "2 2\n",
      "2 1\n",
      "1 2\n",
      "2 2\n",
      "2 0\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "2 0\n",
      "2 2\n",
      "0 2\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "0 2\n",
      "2 1\n",
      "2 0\n",
      "2 0\n",
      "2 2\n",
      "2 0\n",
      "2 0\n",
      "2 2\n",
      "2 2\n",
      "2 0\n",
      "2 2\n",
      "2 2\n",
      "2 0\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 1\n",
      "2 1\n",
      "2 0\n",
      "2 2\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "2 2\n",
      "2 2\n",
      "1 2\n",
      "2 2\n",
      "2 0\n",
      "2 0\n",
      "2 0\n",
      "0 2\n",
      "0 2\n",
      "0 2\n",
      "2 1\n",
      "2 1\n",
      "2 0\n",
      "1 2\n",
      "0 2\n",
      "2 2\n",
      "2 2\n",
      "2 0\n",
      "2 1\n",
      "2 0\n",
      "0 2\n",
      "2 2\n",
      "2 2\n",
      "2 0\n",
      "2 2\n",
      "2 2\n",
      "1 2\n",
      "2 1\n",
      "2 2\n",
      "2 1\n",
      "2 2\n",
      "2 1\n",
      "2 1\n",
      "2 2\n",
      "2 0\n",
      "0 2\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "2 0\n",
      "2 2\n",
      "2 0\n",
      "2 1\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 0\n",
      "2 2\n",
      "0 2\n",
      "1 2\n",
      "2 2\n",
      "2 0\n",
      "2 1\n",
      "2 0\n",
      "2 0\n",
      "2 2\n",
      "2 0\n",
      "2 1\n",
      "2 2\n",
      "0 2\n",
      "2 1\n",
      "2 0\n",
      "0 2\n",
      "2 0\n",
      "2 1\n",
      "2 0\n",
      "1 2\n",
      "1 2\n",
      "2 2\n",
      "2 2\n",
      "2 0\n",
      "2 0\n",
      "2 2\n",
      "2 2\n",
      "2 0\n",
      "2 1\n",
      "2 0\n",
      "0 2\n",
      "2 2\n",
      "2 1\n",
      "2 1\n",
      "2 2\n",
      "1 2\n",
      "2 0\n",
      "2 2\n",
      "2 1\n",
      "2 2\n",
      "2 2\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "2 0\n",
      "0 2\n",
      "2 2\n",
      "2 1\n",
      "2 0\n",
      "2 2\n",
      "2 1\n",
      "2 0\n",
      "1 2\n",
      "1 2\n",
      "0 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "0 2\n",
      "2 2\n",
      "1 2\n",
      "2 2\n",
      "2 1\n",
      "0 2\n",
      "1 2\n",
      "2 0\n",
      "2 2\n",
      "2 0\n",
      "2 1\n",
      "2 0\n",
      "1 2\n",
      "2 0\n",
      "0 2\n",
      "2 1\n",
      "1 2\n",
      "2 2\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "2 0\n",
      "2 1\n",
      "2 0\n",
      "2 1\n",
      "0 2\n",
      "2 1\n",
      "0 2\n",
      "2 2\n",
      "1 2\n",
      "2 0\n",
      "2 1\n",
      "1 2\n",
      "0 2\n",
      "2 0\n",
      "0 2\n",
      "2 0\n",
      "2 1\n",
      "1 2\n",
      "0 2\n",
      "2 2\n",
      "0 2\n",
      "2 2\n",
      "2 1\n",
      "1 2\n",
      "2 0\n",
      "2 1\n",
      "2 0\n",
      "2 0\n",
      "2 1\n",
      "2 2\n",
      "2 1\n",
      "2 1\n",
      "0 2\n",
      "0 2\n",
      "0 2\n",
      "2 1\n",
      "0 2\n",
      "2 0\n",
      "2 0\n",
      "0 2\n",
      "2 1\n",
      "0 2\n",
      "2 2\n",
      "2 1\n",
      "2 0\n",
      "2 1\n",
      "2 0\n",
      "0 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "0 2\n",
      "1 2\n",
      "2 1\n",
      "0 2\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "2 0\n",
      "1 2\n",
      "2 1\n",
      "2 2\n",
      "2 0\n",
      "1 2\n",
      "2 0\n",
      "2 0\n",
      "2 0\n",
      "2 2\n",
      "0 2\n",
      "2 0\n",
      "2 2\n",
      "2 0\n",
      "2 0\n",
      "0 2\n",
      "2 1\n",
      "0 2\n",
      "1 2\n",
      "2 1\n",
      "2 0\n",
      "0 2\n",
      "2 2\n",
      "0 2\n",
      "2 1\n",
      "2 2\n",
      "2 1\n",
      "2 2\n",
      "2 1\n",
      "2 1\n",
      "2 0\n",
      "2 0\n",
      "2 0\n",
      "1 2\n",
      "0 2\n",
      "2 0\n",
      "2 1\n",
      "0 2\n",
      "2 1\n",
      "2 0\n",
      "2 1\n",
      "2 0\n",
      "0 2\n",
      "0 2\n",
      "2 0\n",
      "2 2\n",
      "2 1\n",
      "2 0\n",
      "2 1\n",
      "1 2\n",
      "2 2\n",
      "2 0\n",
      "2 1\n",
      "1 2\n",
      "0 2\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "2 0\n",
      "1 2\n",
      "0 2\n",
      "2 2\n",
      "2 2\n",
      "0 2\n",
      "2 0\n",
      "2 0\n",
      "2 0\n",
      "2 0\n",
      "2 2\n",
      "2 2\n",
      "2 1\n",
      "1 2\n",
      "0 2\n",
      "2 1\n",
      "2 1\n",
      "0 2\n",
      "2 0\n",
      "2 0\n",
      "2 2\n",
      "0 2\n",
      "2 0\n",
      "0 2\n",
      "2 2\n",
      "2 2\n",
      "0 2\n",
      "1 2\n",
      "2 0\n",
      "1 2\n",
      "2 0\n",
      "2 1\n",
      "2 0\n",
      "2 0\n",
      "2 1\n",
      "2 1\n",
      "2 0\n",
      "2 2\n",
      "2 0\n",
      "2 0\n",
      "2 1\n",
      "2 2\n",
      "0 2\n",
      "2 1\n",
      "0 2\n",
      "2 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22105263157894736"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.argmax(deeplob(testX_CNN).numpy(),axis=1)\n",
    "y = testY_CNN.argmax(axis=1)\n",
    "c1,c2 = 0,0\n",
    "for i in range(len(x)):\n",
    "    if x[i] == 2 or y[i] == 2:\n",
    "        c1+=1\n",
    "        if x[i]==y[i]:\n",
    "            c2+=1\n",
    "        print(x[i],y[i])\n",
    "c2/c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deeplob.load_weights(checkpoint_filepath)\n",
    "pred = deeplob.predict(testX_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.36054421768707484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.36257   0.31313   0.33604       198\n",
      "           1    0.36434   0.24103   0.29012       195\n",
      "           2    0.35764   0.52821   0.42650       195\n",
      "\n",
      "    accuracy                        0.36054       588\n",
      "   macro avg    0.36152   0.36079   0.35089       588\n",
      "weighted avg    0.36152   0.36054   0.35081       588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score:', accuracy_score(np.argmax(testY_CNN, axis=1), np.argmax(pred, axis=1)))\n",
    "print(classification_report(np.argmax(testY_CNN, axis=1), np.argmax(pred, axis=1), digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('envi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7db821986d699a458b6831c207daf06594b2df0f12601a94f5be5a3a8c1e7e9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
